<html><head><title>11.5. Parallel Sort&ndash;Merge</title><title>TeamUnknown</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8" /><link rel="STYLESHEET" type="text/css" href="portals/bvdep/xsltemplates/globalstyle.css" /><link href="includes/searchResults.css" rel="stylesheet" type="text/css" /><link rel="STYLESHEET" type="text/css" href="portals/bvdep/xsltemplates/style.css" /><link rel="STYLESHEET" type="text/css" href="portals/bvdep/xsltemplates/docsafari.css" /><body><table width="100%" border="0" cellspacing="0" cellpadding="0"><td class="docBookTitle"><a href="toc.html"><b>[ Team Unknown ]</b></a></td></table><td align="center"><a name="MainContent"></a><table width="95%"><tr><td align="left" class="v2"><!--Copyright (c) 2002 Safari Tech Books Online--><table width="100%" border="0" cellspacing="0" cellpadding="2"><tr><td valign="middle" class="v2" height="5"><img src="pixel.gif" width="1" height="5" alt="" border="0" /></td></tr><tr><td valign="middle" class="v2"><table cellpadding="0" cellspacing="0" border="0" width="100%"><tr><td align="left"><span style="white-space:nowrap">&nbsp;</span>
                  &nbsp;
                  <span style="white-space:nowrap"> &nbsp;&nbsp;</span>
            &nbsp;<span style="white-space:nowrap">&nbsp;</span></td></tr></table></td><td></td><td valign="middle" class="v2" align="right"> 
          &nbsp;
          <span style="white-space:nowrap"><a target="_self" href="ch11lev1sec4.html" title="Previous section"><img border="0" align="absmiddle" src="btn_prev.gif" alt="Previous section" id="btn_prev" /></a></span>
				
				&nbsp;
				
				<span style="white-space:nowrap"><a target="_self" href="ch11lev2sec1.html" title="Next section"><img border="0" align="absmiddle" src="btn_next.gif" alt="Next section" id="btn_next" /></a></span></td></tr></table><div id="section"><br /><table width="100%" border="0" cellspacing="0" cellpadding="0"><tr><td valign="top">C++ Programming Robert Sedgewick - Princeton University Addison Wesley Professional Algorithms in C++, Parts 1&ndash;4: Fundamentals, Data Structure, Sorting, Searching, Third Edition<a name="ch11lev1sec5"></a>
<h3 id="title-IDAQY25H" class="docSection1Title">11.5. Parallel Sort&ndash;Merge</h3>
<p class="docText">How do we get several independent processors to work together on the same sorting problem? Whether the processors control external memory devices or are complete computer systems, this question is at the heart of algorithm design for high-performance computing systems. The subject of parallel computing has been studied widely in recent years. Many different types of parallel computers have been devised, and many different models for parallel computation have been proposed. The sorting problem is a test case for the effectiveness of both.</p>
<p class="docText">We have already discussed low-level parallelism, in our discussion of sorting networks in <a class="docLink" href="ch11lev1sec2.html#ch11lev1sec2">Section 11.2</a>, where we considered doing a number of compare&ndash;exchange operations at the same time. Now, we discuss a high-level parallel model, where we have a large number of independent general-purpose processors (rather than just comparators) that have access to the same data. Again, we ignore many practical issues, but can examine algorithmic questions in this context.</p>
<p class="docText">The abstract model that we use for parallel processing involves a basic assumption that the file to be sorted is distributed among <span class="docEmphasis">P</span></p><p class="docText">independent processors. We assume that we have</p>
<ul><li><p class="docList"><span class="docEmphasis">N</span> records to be sorted and</p></li><li><p class="docList"><span class="docEmphasis">P</span> processors, each capable of holding <span class="docEmphasis">N/P</span> records</p></li></ul>
<p class="docText">We assign the processors the labels 0, 1, <tt>. . .</tt>, <span class="docEmphasis">P</span> - 1, and assume that the file to be input is in the local memories of the processors (that is, each processor has <span class="docEmphasis">N/P</span> of the records). The goal of the sort is to rearrange the records to put the smallest <span class="docEmphasis">N/P</span> records in processor 0's memory, the next smallest <span class="docEmphasis">N/P</span> records in processor 1's memory, and so forth, in sorted order. As we shall see, there is a tradeoff between <span class="docEmphasis">P</span> and the total running time&ndash;we are interested in quantifying that tradeoff so that we can compare competing strategies.</p>
<p class="docText">This model is one of many possible ones for parallelism, and it has many of the same liabilities with respect to practical applicability as did our model for external sorting (<a class="docLink" href="ch11lev1sec3.html#ch11lev1sec3">Section 11.3</a>). Indeed, it does not address one of the most important issues to be faced in parallel computing: constraints on communication between the processors.</p>
<p class="docText">We shall assume that such communication is far more costly than references to local memory, that it is most efficiently done sequentially, in large blocks. In a sense, processors treat other processors' memory as external storage devices. Again, this high-level abstract model can be regarded as unsatisfactory from a practical standpoint, because it is an oversimplification; and can be regarded as unsatisfactory from a <a name="iddle1234"></a><a name="iddle1287"></a><a name="iddle1909"></a><a name="iddle2375"></a><a name="iddle2402"></a>theoretical standpoint, because it is not fully specified. Still, it provides a framework within which we can develop useful algorithms.</p>
<p class="docText">Indeed, this problem (with these assumptions) provides a convincing example of the power of abstraction, because we can use the same sorting networks that we discussed in <a class="docLink" href="ch11lev1sec2.html#ch11lev1sec2">Section 11.2</a>, by modifying the compare&ndash;exchange abstraction to operate on large blocks of data.</p>
<a name="ch11ex12"></a><h5 id="title-IDAP225H" class="docExampleTitle">Definition 11.2. A <span class="docEmphasis">merging comparator</span> takes as input two sorted files of size <span class="docEmphasis">M</span>, and produces as output two sorted files: one containing the <span class="docEmphasis">M</span> smallest of the 2<span class="docEmphasis">M</span> inputs, and the other containing the <span class="docEmphasis">M</span> largest of the 2<span class="docEmphasis">M</span> inputs</h5><p><table cellspacing="0" width="90%" border="1" cellpadding="5"><tr><td>
<p class="docText">Such an operation is easy to implement: Merge the two input files, and output the first half and the second half of the merged result.</p>
</td></tr></table></p>
<a name="ch11ex13"></a><h5 id="title-IDAK325H" class="docExampleTitle">Property 11.7. We can sort a file of size <span class="docEmphasis">N</span> by dividing it into <span class="docEmphasis">N/M</span> blocks of size <span class="docEmphasis">M</span>, sorting each file, then using a sorting network built with merging comparators</h5><p><table cellspacing="0" width="90%" border="1" cellpadding="5"><tr><td>
<p class="docText">Establishing this fact from the 0&ndash;1 principle is tricky (see <a class="docLink" href="#ch11qa5q1">Exercise 11.61</a>), but tracing through an example, such as the one in <a class="docLink" href="#ch11fig18">Figure 11.18</a>, is a persuasive exercise.</p>
</td></tr></table></p>
<a name="ch11fig18"></a><p><center>
<h5 class="docFigureTitle">Figure 11.18. Block sorting example</h5>
<h5></h5><h5></h5></center></p><p class="docText">This figure shows how we can use the network in <a class="docLink" href="ch11lev1sec2.html#ch11fig04">Figure 11.4</a> to sort blocks of data. The comparators put the small half of the elements in the two input lines out onto the top line and the large half out onto the bottom line. Three parallel steps suffice.</p>
<p class="docText">
<img border="0" alt="" width="375" height="49" src="11fig18.gif" /></p>
<br />
<p class="docText">We refer to the method described in <a class="docLink" href="#ch11ex13">Property 11.7</a> as<span class="docEmphasis">block sorting</span>. We have a number of design parameters to consider before we use the method on a particular parallel machine. Our interest in the method concerns the following performance characteristic:</p>
<a name="ch11ex14"></a><h5 id="title-IDACA35H" class="docExampleTitle">Property 11.8. Block sorting on <span class="docEmphasis">P</span> processors, using Batcher's sort with merging comparators, can sort <span class="docEmphasis">N</span> records in about (lg <span class="docEmphasis">P</span>)<sup>2</sup>/2 <span class="docEmphasis">parallel steps</span></h5><p><table cellspacing="0" width="90%" border="1" cellpadding="5"><tr><td>
<p class="docText">By <span class="docEmphasis">parallel step</span> in this context, we mean a set of disjoint merging comparators. <a class="docLink" href="#ch11ex14">Property 11.8</a> is a direct consequence of <a class="docLink" href="ch11lev1sec2.html#ch11ex07">Properties 11.3</a> and <a class="docLink" href="#ch11ex13">11.7</a>.</p>
<p class="docText">To implement a merging comparator on two processors, we can have them exchange copies of their blocks of data, both do the merge (in parallel), and one keep the small half of the keys and the other keep the large half of the keys. If block transfer is slow compared to the individual processor speeds, then we can estimate the total time required for the sort by multiplying the cost of one block transfer by (lg <span class="docEmphasis">P</span>)<sup>2</sup>/2. This estimate embodies a large number of assumptions; for example, it assumes that multiple block transfers can be done in parallel without penalty, a rarely achieved goal in real parallel computers. Still, it provides a starting point for understanding what we can expect in a practical implementation.</p>
<p class="docText">If the block-transfer cost is comparable to individual processor speeds (another ideal goal that is only approached in real machines), then we have to account for the time to do the initial sorts. The processors each do about (<span class="docEmphasis">N/P</span>)lg(<span class="docEmphasis">N/P</span>) comparisons (in parallel) to sort the <span class="docEmphasis">N/P</span> blocks initially, and about <span class="docEmphasis">P</span><sup>2</sup>(lg <span class="docEmphasis">P</span>)/2 stages with (<span class="docEmphasis">N/P</span>)-by-(<span class="docEmphasis">N/P</span>) merges. If the cost of a comparison is &#945; and the cost per record for a merge is &#946;, then the total running time is about</p>
<div class="docText"><pre>&#945;(N/P)lg(N/P) + &#946;(N/P)P<sup>2</sup>(lg P)/2.</pre></div><br />
<p class="docText">For huge <span class="docEmphasis">N</span> and small <span class="docEmphasis">P</span>, this performance is the best that we can hope for in any comparison-based parallel sorting method, because the cost in that case is about &#945;(<span class="docEmphasis">N</span>lg<span class="docEmphasis">N</span>)/<span class="docEmphasis">P</span>, which is optimal: Any sort requires <span class="docEmphasis">N</span>lg<span class="docEmphasis">N</span> comparisons, and the best that we could do is to do <span class="docEmphasis">P</span> of them at once. For large <span class="docEmphasis">P</span>, the second term dominates, and the cost is about &#946;<span class="docEmphasis">N</span>(<span class="docEmphasis">P</span> lg <span class="docEmphasis">P</span>)/2, which is suboptimal but still perhaps is competitive. For example, the second term contributes about 256&#946;<span class="docEmphasis">N/P</span> to the cost of sorting 1 billion elements on 64 processors, as compared to the contribution of 32&#945;<span class="docEmphasis">N/P</span> from the first term.</p>
<p class="docText">When <span class="docEmphasis">P</span> is large, the communication among all the processors might create a bottleneck on some machines. If so, using a perfect shuffle as in <a class="docLink" href="ch11lev1sec2.html#ch11fig08">Figure 11.8</a> might provide a way to control such costs. Some parallel machines have built-in low-level interconnections that allow us to implement shuffles efficiently, for precisely this reason.</p>
<p class="docText">This example shows that we <span class="docEmphasis">can</span> get a large number of processors to work efficiently on a huge sort problem, under certain circumstances. To find the best way to do so, we certainly would need to consider many other algorithms for this kind of parallel machine, to learn many other characteristics of a real parallel machine, and to consider many variations on the machine model that we are using.</p>
<p class="docText">Moreover, we might need to take a completely different approach to parallelism. Still, the idea that increasing the number of processors increases the costs of communicating among them is fundamental to parallel computing, and Batcher's networks provide an effective way of controlling these costs, as we have seen at a low level in <a class="docLink" href="ch11lev1sec2.html#ch11lev1sec2">Section 11.2</a> and at a high level in this section.</p>
<p class="docText">The sorting methods described in this section and elsewhere in this chapter have a flavor different from those of the methods that we have discussed in <a class="docLink" href="ch06.html#ch06">Chapters 6</a> through <a class="docLink" href="ch10.html#ch10">10</a>, because they involve coping with constraints that we do not consider in ordinary programming. In <a class="docLink" href="ch06.html#ch06">Chapters 6</a> through <a class="docLink" href="ch10.html#ch10">10</a>, simple assumptions about the nature of our data were sufficient to allow us to compare a large number of different methods for the same basic problem. By contrast, in this chapter we have focused on articulating a variety of problems, and have been able to discuss just a few solutions for each. These examples illustrate that changes in real-world constraints can provide new opportunities for algorithmic solutions, and a critical part of the process is to develop useful abstract formulations of problems.</p>
<p class="docText">Sorting is essential in many practical applications, and the design of an efficient sort is often one of the first problems to be addressed on new computer architectures and in new programming environments. To the extent that new developments build on past experience, the array of techniques that we have discussed here and in <a class="docLink" href="ch06.html#ch06">Chapters 6</a> through <a class="docLink" href="ch10.html#ch10">10</a> is important to know; to the extent that radical new departures are invented, the kind of abstract thinking discussed here will be necessary if we are to develop fast sorting procedures on new machines.</p>
</td></tr></table></p>
<p class="docText">&nbsp;</p>
<p class="docQandasetTitle">Exercises</p><p><table border="0" cellspacing="16" cellpadding="0"><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa5q1"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="8" height="9" src="circle.jpg" /> <span class="docEmphStrong">11.61</span> Use the 0&ndash;1 principle (<a class="docLink" href="ch11lev1sec1.html#ch11ex04">Property 11.1</a>) to prove <a class="docLink" href="#ch11ex13">Property 11.7</a>.</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa5q2"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="8" height="8" src="blackcircle.jpg" /> <span class="docEmphStrong">11.62</span> Implement a sequential version of block sorting with Batcher's odd&ndash;even merge: (<span class="docEmphasis">i</span>) use standard mergesort (<a class="docLink" href="ch08lev1sec3.html#ch08ex03">Programs 8.3</a> and <a class="docLink" href="ch08lev1sec2.html#ch08ex02">8.2</a>) to sort the blocks, (<span class="docEmphasis">ii</span>) use the standard abstract in-place merge (<a class="docLink" href="ch08lev1sec2.html#ch08ex02">Program 8.2</a>) to implement the merging comparators, and (<span class="docEmphasis">iii</span>) use bottom-up Batcher's odd&ndash;even merge (<a class="docLink" href="ch11lev1sec2.html#ch11ex06">Program 11.3</a>) to implement the block sort.</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa5q3"></a><b></b></td><td><p class="docText"><span class="docEmphStrong">11.63</span> Estimate the running time of the program described in <a class="docLink" href="#ch11qa5q2">Exercise 11.62</a>, as a function of <span class="docEmphasis">N</span> and <span class="docEmphasis">M</span>, for large <span class="docEmphasis">N</span>.</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa5q4"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="8" height="8" src="blackcircle.jpg" /> <span class="docEmphStrong">11.64</span> Do <a class="docLink" href="#ch11qa5q2">Exercises 11.62</a> and <a class="docLink" href="#ch11qa5q3">11.63</a>, but substitute bottom-up Batcher's odd&ndash;even merge (<a class="docLink" href="ch11lev1sec2.html#ch11ex06">Program 11.3</a>) for <a class="docLink" href="ch08lev1sec3.html#ch08ex03">Program 8.2</a> in both instances.</p></td></tr><tr><td></td><td></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa5q5"></a><b></b></td><td><p class="docText"><span class="docEmphStrong">11.65</span> Give the values of <span class="docEmphasis">P</span> for which (<span class="docEmphasis">N/P</span>)lg<span class="docEmphasis">N</span> = <span class="docEmphasis">NP</span> lg <span class="docEmphasis">P</span>, for <span class="docEmphasis">N</span> = 10<sup>3</sup>, 10<sup>6</sup>, 10<sup>9</sup>, and 10<sup>12</sup>.</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa5q6"></a><b></b></td><td><p class="docText"><span class="docEmphStrong">11.66</span> Give approximate expressions of the form <span class="docEmphasis">c</span><sub>1</sub><span class="docEmphasis">N</span> lg <span class="docEmphasis">N</span> + <span class="docEmphasis">c</span><sub>2</sub><span class="docEmphasis">N</span> for the number of comparisons between data items used by a parallel Batcher's block sort, for <span class="docEmphasis">P</span> = 1, 4, 16, 64, and 256.</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa5q7"></a><b></b></td><td><p class="docText"><span class="docEmphStrong">11.67</span> How many parallel steps would be required to sort 10<sup>15</sup> records that are distributed on 1000 disks, using 100 processors?</p></td></tr></table></p>
<ul></ul></td></tr></table><table width="100%" border="0" cellspacing="0" cellpadding="2"><tr><td valign="middle" class="v2" height="5"><img src="pixel.gif" width="1" height="5" alt="" border="0" /></td></tr><tr><td valign="middle" class="v2"><table cellpadding="0" cellspacing="0" border="0" width="100%"><tr><td align="left"><span style="white-space:nowrap">&nbsp;</span>
                  &nbsp;
                  <span style="white-space:nowrap"> &nbsp;&nbsp;</span>
            &nbsp;<span style="white-space:nowrap">&nbsp;</span></td></tr></table></td><td></td><td valign="middle" class="v2" align="right"> 
          &nbsp;
          <span style="white-space:nowrap"><a target="_self" href="ch11lev1sec4.html" title="Previous section"><img border="0" align="absmiddle" src="btn_prev.gif" alt="Previous section" id="btn_prev" /></a></span>
				
				&nbsp;
				
				<span style="white-space:nowrap"><a target="_self" href="ch11lev2sec1.html" title="Next section"><img border="0" align="absmiddle" src="btn_next.gif" alt="Next section" id="btn_next" /></a></span></td></tr></table><table width="100%" border="0" cellspacing="0" cellpadding="2"><tr><td valign="top" align="right"><span style="white-space:nowrap"><a target="_self" href="#toppage" title="Top"></a></span></td></tr></table></div><!--IP User 2--></td></tr></table></td></body></head></html>