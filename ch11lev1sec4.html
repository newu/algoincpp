<html><head><title>11.4. Sort&ndash;Merge Implementations</title><title>TeamUnknown</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8" /><link rel="STYLESHEET" type="text/css" href="portals/bvdep/xsltemplates/globalstyle.css" /><link href="includes/searchResults.css" rel="stylesheet" type="text/css" /><link rel="STYLESHEET" type="text/css" href="portals/bvdep/xsltemplates/style.css" /><link rel="STYLESHEET" type="text/css" href="portals/bvdep/xsltemplates/docsafari.css" /><body><table width="100%" border="0" cellspacing="0" cellpadding="0"><td class="docBookTitle"><a href="toc.html"><b>[ Team Unknown ]</b></a></td></table><td align="center"><a name="MainContent"></a><table width="95%"><tr><td align="left" class="v2"><!--Copyright (c) 2002 Safari Tech Books Online--><table width="100%" border="0" cellspacing="0" cellpadding="2"><tr><td valign="middle" class="v2" height="5"><img src="pixel.gif" width="1" height="5" alt="" border="0" /></td></tr><tr><td valign="middle" class="v2"><table cellpadding="0" cellspacing="0" border="0" width="100%"><tr><td align="left"><span style="white-space:nowrap">&nbsp;</span>
                  &nbsp;
                  <span style="white-space:nowrap"> &nbsp;&nbsp;</span>
            &nbsp;<span style="white-space:nowrap">&nbsp;</span></td></tr></table></td><td></td><td valign="middle" class="v2" align="right"> 
          &nbsp;
          <span style="white-space:nowrap"><a target="_self" href="ch11lev1sec3.html" title="Previous section"><img border="0" align="absmiddle" src="btn_prev.gif" alt="Previous section" id="btn_prev" /></a></span>
				
				&nbsp;
				
				<span style="white-space:nowrap"><a target="_self" href="ch11lev1sec5.html" title="Next section"><img border="0" align="absmiddle" src="btn_next.gif" alt="Next section" id="btn_next" /></a></span></td></tr></table><div id="section"><br /><table width="100%" border="0" cellspacing="0" cellpadding="0"><tr><td valign="top">C++ Programming Robert Sedgewick - Princeton University Addison Wesley Professional Algorithms in C++, Parts 1&ndash;4: Fundamentals, Data Structure, Sorting, Searching, Third Edition<a name="ch11lev1sec4"></a>
<h3 id="title-IDAD0XFI" class="docSection1Title">11.4. Sort&ndash;Merge Implementations</h3>
<p class="docText">The general sort&ndash;merge strategy outlined in <a class="docLink" href="ch11lev1sec3.html#ch11lev1sec3">Section 11.3</a> is effective in practice. In this section, we consider two improvements that can lower the costs. The first technique, <span class="docEmphasis">replacement selection</span>, has the same effect on the running time as does increasing the amount of internal memory that we use; the second technique, <span class="docEmphasis">polyphase merging</span>, has the same effect as does increasing the number of devices that we use.</p>
<p class="docText">In <a class="docLink" href="ch11lev1sec3.html#ch11lev1sec3">Section 11.3</a>, we discussed the use of priority queues for <span class="docEmphasis">P</span>-way merging, but noted that <span class="docEmphasis">P</span> is so small that fast algorithmic improvements are unimportant. During the initial distribution pass, however, we can make good use of fast priority queues to produce sorted runs that are longer than could fit in internal memory. The idea is to pass the (unordered) input through a large priority queue, always writing out the smallest element on the priority queue as before, and always replacing it with the next element from the input, with one additional <a name="iddle1912"></a><a name="iddle2420"></a>proviso: If the new element is smaller than the one output most recently, then, because it could not possibly become part of the current sorted block, we mark it as a member of the next block and treat it as greater than all elements in the current block. When a marked element makes it to the top of the priority queue, we begin a new block. <a class="docLink" href="#ch11fig14">Figure 11.14</a> depicts the method in operation.</p>
<a name="ch11fig14"></a><p><center>
<h5 class="docFigureTitle">Figure 11.14. Replacement selection</h5>
<h5></h5><h5></h5></center></p><p class="docText">This sequence shows how we can produce the two runs A<span class="docEmphasis"><tt>I N O R S T X and A E E G L M P</tt></span>, which are of length 8 and 7, respectively, from the sequence <span class="docEmphasis"><tt>A S O R T I N G E X A M P L E</tt></span> using a heap of size 5.</p>
<p class="docText">
<img border="0" alt="" width="75" height="523" src="11fig14.gif" /></p>
<br />
<a name="ch11ex10"></a><h5 id="title-IDAH3XFI" class="docExampleTitle">Property 11.5. For random keys, the runs produced by replacement selection are about twice the size of the heap used</h5><p><table cellspacing="0" width="90%" border="1" cellpadding="5"><tr><td>
<p class="docText">If we were to use heapsort to produce initial runs, we would fill the memory with records, then write them out one by one, continuing until the heap is empty. Then, we would fill the memory with another batch of records and repeat the process, again and again. On the average, the heap occupies only one-half the memory during this process. By contrast, replacement selection keeps the memory filled with the same data structure, so it is not surprising that it does twice as well. The full proof of this property requires a sophisticated analysis (<span class="docEmphasis">see reference section</span>), although the property is easy to verify experimentally (see <a class="docLink" href="#ch11qa4q3">Exercise 11.47</a>).</p>
</td></tr></table></p>
<p class="docText">For random files, the practical effect of replacement selection is to save perhaps one merging pass: Rather than starting with sorted runs about the size of the internal memory, then taking a merging pass to produce longer runs, we can start right off with runs about twice the size of the internal memory. For <span class="docEmphasis">P</span> = 2, this strategy would save precisely one merging pass; for larger <span class="docEmphasis">P</span>, the effect is less important. However, we know that practical sorts rarely deal with random files, and, if there is some order in the keys, then using replacement selection could result in huge runs. For example, if no key has more than <span class="docEmphasis">M</span> larger keys before it in the file, the file will be completely sorted by the replacement-selection pass, and no merging will be necessary! This possibility is the most important practical reason to use replacement selection.</p>
<p class="docText">The major weakness of balanced multiway merging is that only about one-half the devices are actively in use during the merges: the <span class="docEmphasis">P</span> input devices and whichever device is collecting the output. An alternative is always to do (2<span class="docEmphasis">P</span> - 1)-way merges with all output onto device 0, then distribute the data back to the other tapes at the end of each merging pass. But this approach is not more efficient, because <a name="iddle1913"></a><a name="iddle2081"></a><a name="iddle2422"></a>it effectively doubles the number of passes, for the distribution. Balanced multiway merging seems to require either an excessive number of tape units or excessive copying. Several clever algorithms have been invented that keep all the external devices busy by changing the way in which the small sorted blocks are merged together. The simplest of these methods is called <span class="docEmphasis">polyphase merging</span>.</p>
<p class="docText">The basic idea behind polyphase merging is to distribute the sorted blocks produced by replacement selection somewhat unevenly among the available tape units (leaving one empty) and then to apply a <span class="docEmphasis">merge-until-empty</span> strategy: Since the tapes being merged are of unequal length, one will run out sooner that the rest, and it then can be used as output. That is, we switch the roles of the output tape (which now has some sorted blocks on it) and the now-empty input tape, continuing the process until only one block remains. <a class="docLink" href="#ch11fig15">Figure 11.15</a> depicts an example.</p>
<a name="ch11fig15"></a><p><center>
<h5 class="docFigureTitle">Figure 11.15. Polyphase merge example</h5>
<h5></h5><h5></h5></center></p><p class="docText">In the initial distribution phase, we put the different numbers of runs on the tapes according to a prearranged scheme, rather than keeping the numbers of runs balanced, as we did in <a class="docLink" href="ch11lev1sec3.html#ch11fig12">Figure 11.12</a>. Then, we do three-way merges at every phase until the sort is complete. There are more phases than for the balanced merge, but the phases do not involve all the data.</p>
<p class="docText">
<img border="0" alt="" width="500" height="201" src="11fig15.gif" /></p>
<br />
<p class="docText">The merge-until-empty strategy works for an arbitrary number of tapes, as shown in <a class="docLink" href="#ch11fig16">Figure 11.16</a>. The merge is broken up into many <span class="docEmphasis">phases</span>, not all of which involve all of the data, and which involve no extra copying. <a class="docLink" href="#ch11fig16">Figure 11.16</a> shows how to compute the initial run distribution. We compute the <span class="docEmphasis">number</span> of runs on each device by working backward.</p>
<a name="ch11fig16"></a><p><center>
<h5 class="docFigureTitle">Figure 11.16. Run distribution for polyphase three-way merge</h5>
<h5></h5><h5></h5></center></p><p class="docText">In the initial distribution for a polyphase three-way merge of a file 17 times the size of the internal memory, we put seven runs on device 0, four runs on device 2, and six runs on device 3. Then, in the first phase, we merge until device 2 is empty, leaving three runs of size 1 on device 0, two runs of size 1 on device 3, and creating four runs of size 3 on device 1. For a file 15 times the size of the internal memory, we put 2 dummy runs on device 0 at the beginning (see <a class="docLink" href="#ch11fig15">Figure 11.15</a>). The total number of blocks processed for the whole merge is 59, one fewer than for our balanced merging example (see <a class="docLink" href="ch11lev1sec3.html#ch11fig13">Figure 11.13</a>), but we use two fewer devices (see also <a class="docLink" href="#ch11qa4q6">Exercise 11.50</a>).</p>
<p class="docText">
<img border="0" alt="" width="175" height="106" src="11fig16.gif" /></p>
<br />
<p class="docText">For the example depicted in <a class="docLink" href="#ch11fig16">Figure 11.16</a>, we reason as follows: We want to finish the merge with 1 run, on device 0. Therefore, just <a name="iddle1478"></a><a name="iddle1545"></a>before the last merge, we want device 0 to be empty, and we want to have 1 run on each of devices 1, 2, and 3. Next, we deduce the run distribution that we would need just before the next-to-last merge for that merge to produce this distribution. One of devices 1, 2, or 3 has to be empty (so that it can be the output device for the next-to-last merge)&mdash;we pick 3 arbitrarily. That is, the next-to-last merge merges together 1 run from each of devices 0, 1, and 2, and puts the result on device 3. Since the next-to-last merge <span class="docEmphasis">leaves</span> 0 runs on device 0 and 1 run on each of devices 1 and 2, it must have begun with 1 run on device 0 and 2 runs on each of devices 1 and 2. Similar reasoning tells us that the merge prior to that must have begun with 2, 3, and 4 runs on devices 3, 0, and 1, respectively. Continuing in this fashion, we can build the table of run distributions: Take the largest number in each row, make it zero, and add it to each of the other numbers to get the previous row. This convention corresponds to defining for the previous row the highest-order merge that could give the present row. This technique works for any number of tapes (at least three): The numbers that arise are <span class="docEmphasis">generalized Fibonacci numbers</span>, which have many interesting properties. If the number of runs is not a generalized Fibonacci number, we assume the existence of dummy runs to make the number of initial runs exactly what is needed for the table. The main challenge in implementing a polyphase merge is to determine how to distribute the initial runs (see <a class="docLink" href="#ch11qa4q10">Exercise 11.54</a>).</p>
<p class="docText">Given the run distribution, we can compute the relative lengths of the runs by working forward, keeping track of the run lengths produced by the merges. For example, the first merge in the example in <a class="docLink" href="#ch11fig16">Figure 11.16</a> produces 4 runs of relative size 3 on device 0, leaving 2 runs of size 1 on device 2 and 1 run of size 1 on device 3, and so forth. As we did for balanced multiway merging, we can perform the indicated multiplications, sum the results (not including the bottom row), and divide by the number of initial runs to get a measure of the cost as a multiple of the cost of making a full pass over all the data. For simplicity, we include the dummy runs in the cost calculation, which gives us an upper bound on the true cost.</p>
<a name="ch11ex11"></a><h5 id="title-IDASECWD" class="docExampleTitle">Property 11.6. With three external devices and internal memory sufficient to hold <span class="docEmphasis">M</span> records, a sort&ndash;merge that is based on replacement selection followed by a two-way polyphase merge takes about 1 + <img src="U2308.GIF" border="0" />log<sub>&#248;</sub>(<span class="docEmphasis">N</span>/2<span class="docEmphasis">M</span>)<img src="U2309.GIF" border="0" />/&#248; effective passes, on the average</h5><p><table cellspacing="0" width="90%" border="1" cellpadding="5"><tr><td>
<p class="docText">The general analysis of polyphase merging, done by <a class="docLink" href="ch11lev2sec1.html#biblio11_008">Knuth</a> and other researchers in the 1960s and 1970s, is complicated, extensive, and beyond the scope of this book. For <span class="docEmphasis">P</span> = 3, the Fibonacci numbers are involved&ndash;hence the appearance of &#248;. Other constants arise for larger <span class="docEmphasis">P</span>. The factor 1/&#248; accounts for the fact that each phase involves only that fraction of the data. We count the number of &quot;effective passes&quot; as the amount of data read divided by the total amount of data. Some of the general research results are surprising. For example, the optimal method for distributing dummy runs among the tapes involves using extra phases and more dummy runs than would seem to be needed, because some runs are used in merges much more often than are others (<span class="docEmphasis">see reference section</span>).</p>
</td></tr></table></p>
<p class="docText"><a name="iddle1746"></a><a name="iddle1876"></a><a name="iddle2424"></a><a name="iddle2615"></a>For example, if we want to sort 1 billion records using three devices and enough internal memory to hold 1 million records, we can do so with a two-way polyphase merge with <img src="U2308.GIF" border="0" />log<sub>&#248;</sub> 500<img src="U2309.GIF" border="0" />/&#248; = 8 passes. Adding the distribution pass, we incur a slightly higher cost (one pass) than a balanced merge that uses <span class="docEmphasis">twice</span> as many devices. That is, we can think of the polyphase merge as enabling us to do the same job with half the amount of hardware. For a given number of devices, polyphase is always more efficient than balanced merging, as indicated in <a class="docLink" href="#ch11fig17">Figure 11.17</a>.</p>
<a name="ch11fig17"></a><p><center>
<h5 class="docFigureTitle">Figure 11.17. Balanced and polyphase merge cost comparisons</h5>
<h5></h5><h5></h5></center></p><p class="docText">The number of passes used in balanced merging with 4 tapes <span class="docEmphasis">(top)</span> is always larger than the number of effective passes used in polyphase merging with 3 tapes <span class="docEmphasis">(bottom)</span>. These plots are drawn from the functions in <a class="docLink" href="ch11lev1sec3.html#ch11ex09">Properties 11.4</a> and <a class="docLink" href="#ch11ex11">11.6</a>, for N/M from 1 to 100. Because of dummy runs, the true performance of polyphase merging is more complicated than indicated by this step function.</p>
<p class="docText">
<img border="0" alt="" width="130" height="91" src="11fig17.gif" /></p>
<br />
<p class="docText">As we discussed at the beginning of <a class="docLink" href="ch11lev1sec3.html#ch11lev1sec3">Section 11.3</a>, our focus on an abstract machine with sequential access to external devices has allowed us to separate algorithmic issues from practical issues. While developing practical implementations, we need to test our basic assumptions and to take care that they remain valid. For example, we depend on efficient implementations of the input&mdash;output functions that transfer data between the processor and the external devices, and other systems software. Modern systems generally have well-tuned implementations of such software.</p>
<p class="docText">Taking this point of view to an extreme, note that many modern computer systems provide a large <span class="docEmphasis">virtual memory</span> capability&ndash;a more general abstract model for accessing external storage than the one we have been using. In a virtual memory, we have the ability to address a huge number of records, leaving to the system the responsibility of making sure that the addressed data are transferred from external to internal storage when needed; our access to the data is seemingly as convenient as is direct access to the internal memory. But the illusion <a name="iddle2524"></a>is not perfect: As long as a program references memory locations that are relatively close to other recently referenced locations, then transfers from external to internal storage are needed infrequently, and the performance of virtual memory is good. (For example, programs that access data sequentially fall in this category.) If a program's memory accesses are scattered, however, the virtual memory system may <span class="docEmphasis">thrash</span> (spend all its time accessing external memory), with disastrous results.</p>
<p class="docText">Virtual memory should not be overlooked as a possible alternative for sorting huge files. We could implement sort&ndash;merge directly, or, even simpler, could use an internal sorting method such as quicksort or mergesort. These internal sorting methods deserve serious consideration in a good virtual-memory environment. Methods such as heapsort or a radix sort, where the the references are scattered throughout the memory, are not likely to be suitable, because of thrashing.</p>
<p class="docText">On the other hand, using virtual memory can involve excessive overhead, and relying instead on our own, explicit methods (such as those that we have been discussing) may be the best way to get the most out of high-performance external devices. One way to characterize the methods that we have been examining is that they are designed to make as many independent parts of the computer system as possible work at full efficiency, without leaving any part idle. When we consider the independent parts to be processors themselves, we are led to parallel computing, the subject of <a class="docLink" href="ch11lev1sec5.html#ch11lev1sec5">Section 11.5</a>.</p>
<p class="docQandasetTitle">Exercises</p><p><table border="0" cellspacing="16" cellpadding="0"><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q1"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="8" height="9" src="triangle.jpg" /> <span class="docEmphStrong">11.45</span> Give the runs produced by replacement selection with a priority queue of size 4 for the keys E A S Y Q U E S T I O N.</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q2"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="8" height="9" src="circle.jpg" /> <span class="docEmphStrong">11.46</span> What is the effect of using replacement selection on a file that was produced by using replacement selection on a given file?</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q3"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="8" height="8" src="blackcircle.jpg" /> <span class="docEmphStrong">11.47</span> Empirically determine the average number of runs produced using replacement selection with a priority queue of size 1000, for random files of size <span class="docEmphasis">N</span> = 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>.</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q4"></a><b></b></td><td><p class="docText"><span class="docEmphStrong">11.48</span> What is the <span class="docEmphasis">worst-case</span> number of runs when you use replacement selection to produce initial runs in a file of <span class="docEmphasis">N</span> records, using a priority queue of size <span class="docEmphasis">M</span> with <span class="docEmphasis">M &lt; N</span>?</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q5"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="8" height="9" src="triangle.jpg" /> <span class="docEmphStrong">11.49</span> Show how the keys E A S Y Q U E S T I O N W I T H P L E N T Y O F K E Y S are sorted using polyphase merging, in the style of the example diagrammed in <a class="docLink" href="#ch11fig15">Figure 11.15</a>.</p></td></tr><tr><td></td><td></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q6"></a><b></b></td><td><p class="docText"><a name="iddle2421"></a><img border="0" alt="" width="8" height="9" src="circle.jpg" /> <span class="docEmphStrong">11.50</span> In the polyphase merge example of <a class="docLink" href="#ch11fig15">Figure 11.15</a>, we put two dummy runs on the tape with 7 runs. Consider the other ways of distributing the dummy runs on the tapes, and find the one that leads to the lowest-cost merge.</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q7"></a><b></b></td><td><p class="docText"><span class="docEmphStrong">11.51</span> Draw a table corresponding to <a class="docLink" href="ch11lev1sec3.html#ch11fig13">Figure 11.13</a> to determine the largest number of runs that could be merged by balanced three-way merging with five passes through the data (using six devices).</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q8"></a><b></b></td><td><p class="docText"><span class="docEmphStrong">11.52</span> Draw a table corresponding to <a class="docLink" href="#ch11fig16">Figure 11.16</a> to determine the largest number of runs that could be merged by polyphase merging at the same cost as five passes through all the data (using six devices).</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q9"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="8" height="9" src="circle.jpg" /> <span class="docEmphStrong">11.53</span> Write a program to compute the number of passes used for multiway merging and the effective number of passes used for polyphase merging for a given number of devices and a given number of initial blocks. Use your program to print a table of these costs for each method, for <span class="docEmphasis">P</span> = 3, 4, 5, 10, and 100, and <span class="docEmphasis">N</span> = 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>.</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q10"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="17" height="8" src="douleblackcircle.jpg" /> <span class="docEmphStrong">11.54</span> Write a program to assign initial runs to devices for <span class="docEmphasis">P</span>-way polyphase merging, sequentially. Whenever the number of runs is a generalized Fibonacci number, the runs should be assigned to devices as required by the algorithm; your task is to find a convenient way to distribute the runs, one at a time.</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q11"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="17" height="8" src="douleblackcircle.jpg" /> <span class="docEmphStrong">11.55</span> Implement replacement selection using the interface defined in <a class="docLink" href="ch11lev1sec3.html#ch11qa3q4">Exercise 11.38</a>.</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q12"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="17" height="8" src="douleblackcircle.jpg" /> <span class="docEmphStrong">11.56</span> Combine your solutions to <a class="docLink" href="ch11lev1sec3.html#ch11qa3q4">Exercise 11.38</a> and <a class="docLink" href="#ch11qa4q11">Exercise 11.55</a> to make a sort&ndash;merge implementation. Use your program to sort as large a file as is feasible on your system, using polyphase merging. If possible, determine the effect on the running time of increasing the number of devices.</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q13"></a><b></b></td><td><p class="docText"><span class="docEmphStrong">11.57</span> How should small files be handled in a quicksort implementation to be run on a huge file in a virtual-memory environment?</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q14"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="17" height="8" src="douleblackcircle.jpg" /> <span class="docEmphStrong">11.58</span> If your computer has a suitable virtual memory system, empirically compare quicksort, LSD radix sort, MSD radix sort, and heapsort for huge files. Use as large a file size as is feasible.</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q15"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="17" height="8" src="douleblackcircle.jpg" /> <span class="docEmphStrong">11.59</span> Develop an implementation for recursive multiway mergesort based on <span class="docEmphasis">k</span>-way merging that would be suitable for sorting huge files in a virtual-memory environment (see <a class="docLink" href="ch08lev1sec3.html#ch08qa3q3">Exercise 8.11</a>).</p></td></tr><tr valign="top"><td align="right" class="docText" width="50"><a name="ch11qa4q16"></a><b></b></td><td><p class="docText"><img border="0" alt="" width="17" height="8" src="douleblackcircle.jpg" /> <span class="docEmphStrong">11.60</span> If your computer has a suitable virtual memory system, empirically determine the value of <span class="docEmphasis">k</span> that leads to the lowest running time for your implementation for <a class="docLink" href="#ch11qa4q15">Exercise 11.59</a>. Use as large a file size as is feasible.</p></td></tr></table></p>
<ul></ul></td></tr></table><table width="100%" border="0" cellspacing="0" cellpadding="2"><tr><td valign="middle" class="v2" height="5"><img src="pixel.gif" width="1" height="5" alt="" border="0" /></td></tr><tr><td valign="middle" class="v2"><table cellpadding="0" cellspacing="0" border="0" width="100%"><tr><td align="left"><span style="white-space:nowrap">&nbsp;</span>
                  &nbsp;
                  <span style="white-space:nowrap"> &nbsp;&nbsp;</span>
            &nbsp;<span style="white-space:nowrap">&nbsp;</span></td></tr></table></td><td></td><td valign="middle" class="v2" align="right"> 
          &nbsp;
          <span style="white-space:nowrap"><a target="_self" href="ch11lev1sec3.html" title="Previous section"><img border="0" align="absmiddle" src="btn_prev.gif" alt="Previous section" id="btn_prev" /></a></span>
				
				&nbsp;
				
				<span style="white-space:nowrap"><a target="_self" href="ch11lev1sec5.html" title="Next section"><img border="0" align="absmiddle" src="btn_next.gif" alt="Next section" id="btn_next" /></a></span></td></tr></table><table width="100%" border="0" cellspacing="0" cellpadding="2"><tr><td valign="top" align="right"><span style="white-space:nowrap"><a target="_self" href="#toppage" title="Top"></a></span></td></tr></table></div><!--IP User 2--></td></tr></table></td></body></head></html>